На построенных графиках зависимости вероятности правильного токена от номера
слоя можно увидеть некоторый повторяющийся сценарий предсказания:
1. Сначала идет плоский участок с маленькой вероятностью, у которого есть
некоторый скачок в самом начале (1-2 слой) и скачок на 5 слое;
2. Вторым идет резкий рост, начинающиейся с 15-18 слоя и заканчивающийся
примерно на 20 (хотя на одном из 9 примеров скачок не так сильно выражен, для 
предсказания man, но рост также начинается с 17);
3. Последний участок, который появляется после резкого скачка, не имеет 
такого выраженного постоянства как первый, но на графике зависимости энтропии
она есть, на последнем слое есть некоторый скачок, добавляющий неуверенность 
в ответе. Думаю, что модель таким образом диверсифицирует вероятность ответа
между подходящими токенами. Можно посмотреть на предсказание токена для 
бананов, на нем видно, что вплоть до последнего слоя модель была уверена в 
токене ban, но в последний момент поменяла решение в пользу bunch.

На графиках зависимости энтропии от номера слоя можно видеть ожидаемую картину
того, что он находится в обратной зависимости с графиком вероятности. Энтропия
характеризует уверенность модели в ответе.

График зависимости косинусного расстояния между последним слоем и 
промежуточными слоями модели от номера слоя не привносит в понимание что-то 
особенно важное. На нем видно примерно монотонное возрастание косинусной 
схожести. Единственное, что можно отметить это повторение паттерна в самом 
начале (0-5 слой), в начале графика для вероятности наблюдается нечто похожее.

Более интересным является график усредненного по головам внимания и по токенам.
В нем можно увидеть явно повторяющиеся участки для всех изображений.

1. В самом начале можно увидеть некоторую аномальную величину: на графиках для
внимания изображения и обменного внимания можно увидеть сразу большое значение,
которое быстро падает буквально за несколько слоев, на графике же для текста 
наблюдается пик (значение становится быстро большим и стремительно падает);
2. Затем идет небольшой плавный максимум с максимальными значениями в районе 15 
слоя для текстового и обменного внимания и в районе 10 для внимания изображения;
3. Последним идет примерно плоский участок с максимумом на последнем слое, 
что интересно это происходит для всех видов внимания одновременно и тоже с 
резким скачком как на прошлых графиках для вероятности и энтропии. Можно 
предположить, что для уточняющего диверсифицирующего скачка происходит активное
использование внимания для обеих модальностей, чтобы получить более подходящий 
ответ.

На графиках для внимания можно увидеть интересную картину того, что внимание
изображения является некоторым ведущим для модели - видно, что в целом паттерны
всех видов внимания похожи (они имеют аномалии на концах и плавный максимум в 
центре), но если смотреть более внимательно на номера слоев, то можно заметить,
что внимание изображения переживает пик в начале и максимум в центре раньше.
Думаю, что это говорит о догоняющем поведении информации в логитах текста, 
информация раньше появляется в логитах изображения и только следом перетекает
в логиты текста. На последнем участке уже нельзя так уверенно заметить 
опаздывание, но также можно увидеть паттерн в виде букв М, который появляется 
раньше для изображения. Можно предположить, что последний слой в некотором 
смысле занимается синхронизацией модальностей для конечной выдачи.

Для аномального поведения модели в начале можно выдвинуть предположение: 
возможно пик возникает в связи с архитектурной неоднородностью модели 
в самом начале. VIT, в котором информация исключительно для изображения
предобрабатывается для входа в LLM, на входе есть, а такого же трансформерного
обработчика для текста нет, поэтому имеется большое накопление "знаний" VIT в
самом первом слое модели, которое стремится быстро перетечь в текстовые логиты.
Этим же возможно можно объяснить и запаздывание для текста.